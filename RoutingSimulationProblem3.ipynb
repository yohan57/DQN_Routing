{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a27823e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/sjy/anaconda3/envs/jjs2/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:463: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/sjy/anaconda3/envs/jjs2/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:464: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/sjy/anaconda3/envs/jjs2/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:465: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/sjy/anaconda3/envs/jjs2/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:466: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/sjy/anaconda3/envs/jjs2/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:467: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n"
     ]
    }
   ],
   "source": [
    "# import requirements\n",
    "from __future__ import print_function\n",
    "import os, sys, time, datetime, json, random\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD , Adam, RMSprop\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab27dc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "### main constants\n",
    "\n",
    "# markers\n",
    "free_cell_mark = 1.0\n",
    "visited_cell_mark = 0.8 \n",
    "agent_cell_mark = 0.5  \n",
    "obstacle_cell_mark = 0.0\n",
    "destination_cell_mark = 0.3\n",
    "\n",
    "# rewards\n",
    "GOAL = 1.0\n",
    "REVISIT = -0.25\n",
    "VALID = -0.04\n",
    "INVALID = -0.8\n",
    "# BLOCKED = -0.75\n",
    "\n",
    "\n",
    "# Actions \n",
    "LEFT = 0\n",
    "UP = 1\n",
    "RIGHT = 2\n",
    "DOWN = 3\n",
    "VIA = 4\n",
    "actions_dict = {\n",
    "    LEFT: 'left',\n",
    "    UP: 'up',\n",
    "    RIGHT: 'right',\n",
    "    DOWN: 'down',\n",
    "    VIA: 'via',\n",
    "}\n",
    "\n",
    "ACTION_NUMS = len(actions_dict)\n",
    "\n",
    "# Exploration factor\n",
    "epsilon = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4299cb4",
   "metadata": {},
   "source": [
    "- free cell : 1.0\n",
    "- visited cell : 0.8\n",
    "- agent cell : 0.5\n",
    "- occupied cell : 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a55aeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Board Environment\n",
    "class Environment(object):\n",
    "    def __init__(self, grid_heigth=12, grid_width=12, target_agent_num=0):\n",
    "        self.height = grid_heigth\n",
    "        self.width = grid_width\n",
    "        self.original_front_layer = np.array([\n",
    "            [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
    "            [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
    "            [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
    "            [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
    "            [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
    "            [ 1.,  1.,  1.,  1.,  0.,  0.,  0.,  0.,  1.,  1.,  1.,  1.],\n",
    "            [ 1.,  1.,  1.,  1.,  0.,  0.,  0.,  0.,  1.,  1.,  1.,  1.],\n",
    "            [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
    "            [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
    "            [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
    "            [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
    "            [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
    "        ])\n",
    "\n",
    "        self.original_back_layer = np.array([\n",
    "            [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
    "            [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
    "            [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
    "            [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
    "            [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
    "            [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
    "            [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
    "            [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
    "            [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
    "            [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
    "            [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
    "            [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
    "        ])\n",
    "\n",
    "        self.start_and_destination = [((2,2,\"front\"),(9,9,\"back\")), ((2,9,\"front\"),(9,2,\"front\"))]\n",
    "        self.via_holes = []\n",
    "        for i in range(self.height):\n",
    "            for j in range(self.width):\n",
    "                self.via_holes.append((i, j))\n",
    "        self.via_done = False\n",
    "        self.agent_num = len(self.start_and_destination)\n",
    "\n",
    "        self.front_layer = self.original_front_layer.copy()\n",
    "        self.back_layer = self.original_back_layer.copy()\n",
    "        self.layers = {\"front\": self.front_layer, \"back\": self.back_layer}\n",
    "\n",
    "        self.size = self.front_layer.size + self.back_layer.size\n",
    "\n",
    "        self.reset(target_agent_num)\n",
    "\n",
    "    def reset(self, agent_num = 0):\n",
    "        self.front_layer = self.original_front_layer.copy()\n",
    "        self.back_layer = self.original_back_layer.copy()\n",
    "        self.layers = {\"front\": self.front_layer, \"back\": self.back_layer}\n",
    "        self.via_done = False\n",
    "\n",
    "        current_start_and_destination = self.start_and_destination[agent_num]\n",
    "        current_start = current_start_and_destination[0]\n",
    "        current_destination = current_start_and_destination[1]\n",
    "        self.start = (current_start[0], current_start[1], current_start[2])\n",
    "        self.destination = (current_destination[0], current_destination[1], current_destination[2])\n",
    "        self.agent = (current_start[0], current_start[1], current_start[2])\n",
    "\n",
    "        for i in range(self.agent_num):\n",
    "            if i == agent_num:\n",
    "                # start, destination cell 마킹\n",
    "                start_and_destination = self.start_and_destination[i]\n",
    "                start = start_and_destination[0]\n",
    "                destination = start_and_destination[1]\n",
    "                self.layers[start[2]][start[0],start[1]] = agent_cell_mark\n",
    "                self.layers[destination[2]][destination[0],destination[1]] = destination_cell_mark\n",
    "            else:\n",
    "                # 다른 agent 의 start, destination : obstacle 취급\n",
    "                start_and_destination_as_obstacle = self.start_and_destination[i]\n",
    "                start_as_obstacle = start_and_destination_as_obstacle[0]\n",
    "                destination_as_obstacle = start_and_destination_as_obstacle[1]\n",
    "                self.layers[start_as_obstacle[2]][start_as_obstacle[0],start_as_obstacle[1]] = obstacle_cell_mark\n",
    "                self.layers[destination_as_obstacle[2]][destination_as_obstacle[0],destination_as_obstacle[1]] = obstacle_cell_mark\n",
    "        \n",
    "        # TODO: to be uncommented - randomly genererate the obstacles\n",
    "#         for _ in range(num_obstacles):\n",
    "#             x, y = np.random.randint(self.grid.shape[0]), np.random.randint(self.grid.shape[1])\n",
    "#             if (x == self.start[0] and y == self.start[1]) or (x == self.destination[0] and y == self.destination[1]):\n",
    "#                 continue\n",
    "#             self.grid[x, y] = obstacle_cell_mark\n",
    "#             if (x,y) in self.free_cells:\n",
    "#                 self.free_cells.remove((x,y))\n",
    "        \n",
    "        \n",
    "        self.min_reward = INVALID * self.height * self.width / 4\n",
    "        self.agent_state = (self.agent[0], self.agent[1], self.agent[2], 'start')\n",
    "        \n",
    "        self.total_reward = 0.0\n",
    "        \n",
    "        self.visited = set()\n",
    "        \n",
    "\n",
    "    def update_state(self, action):\n",
    "        nrow, ncol, nlayer, nmode = agent_row, agent_col, layer, mode = self.agent_state\n",
    "\n",
    "        if self.layers[layer][agent_row, agent_col] > 0.0:\n",
    "            self.visited.add((agent_row, agent_col, layer))  # mark visited cell\n",
    "\n",
    "        valid_actions = self.get_valid_actions()\n",
    "                \n",
    "        if not valid_actions:\n",
    "            nmode = 'blocked'\n",
    "        elif action in valid_actions:\n",
    "            nmode = 'valid'\n",
    "            if action == LEFT:\n",
    "                ncol -= 1\n",
    "            elif action == UP:\n",
    "                nrow -= 1\n",
    "            elif action == RIGHT:\n",
    "                ncol += 1\n",
    "            elif action == DOWN:\n",
    "                nrow += 1\n",
    "            elif action == VIA:\n",
    "                if layer == \"front\" : nlayer = \"back\"\n",
    "                elif layer == \"back\" : nlayer = \"front\"\n",
    "                self.via_done = True\n",
    "        else:                  # invalid action, no change in agent position\n",
    "            nmode = 'invalid' #????\n",
    "\n",
    "        # new state\n",
    "        self.agent_state = (nrow, ncol, nlayer, nmode)\n",
    "        self.layers[self.agent[2]][self.agent[0], self.agent[1]] = visited_cell_mark\n",
    "        self.agent = (nrow, ncol, nlayer)\n",
    "        self.layers[self.agent[2]][self.agent[0], self.agent[1]] = agent_cell_mark\n",
    "\n",
    "    def get_reward(self):\n",
    "        agent_row, agent_col, agent_layer, mode = self.agent_state\n",
    "        if agent_row == self.destination[0] and agent_col == self.destination[1]:\n",
    "            return GOAL\n",
    "        if mode == 'blocked':\n",
    "            return self.min_reward - 1 ###################????\n",
    "        if (agent_row, agent_col, agent_layer) in self.visited:\n",
    "            return REVISIT\n",
    "        if mode == 'invalid':\n",
    "            return INVALID\n",
    "        if mode == 'valid':\n",
    "            return VALID\n",
    "\n",
    "    def act(self, action):\n",
    "        self.update_state(action)\n",
    "        reward = self.get_reward()\n",
    "        self.total_reward += reward\n",
    "        status = self.game_status()\n",
    "        envstate = self.observe()\n",
    "        return envstate, reward, status\n",
    "\n",
    "    def observe(self):\n",
    "        canvas = self.draw_env()\n",
    "        envstate = canvas.reshape((1, -1))\n",
    "        return envstate\n",
    "\n",
    "    def draw_env(self):\n",
    "        canvas = np.array([self.layers[\"front\"], self.layers[\"back\"]])\n",
    "        return canvas\n",
    "\n",
    "    def game_status(self):\n",
    "        if self.total_reward < self.min_reward:\n",
    "            return 'lose'\n",
    "        agent_row, agent_col, agent_layer, mode = self.agent_state\n",
    "        \n",
    "        if agent_row == self.destination[0] and agent_col == self.destination[1]:\n",
    "            return 'win'\n",
    "\n",
    "        return 'not_over'\n",
    "\n",
    "    def get_valid_actions(self, cell=None):\n",
    "        if cell is None:\n",
    "            row, col, layer, mode = self.agent_state\n",
    "        else:\n",
    "            row, col, layer = cell\n",
    "        \n",
    "        actions = [LEFT, UP, RIGHT, DOWN, VIA]\n",
    "        \n",
    "        nrows, ncols = self.height, self.width\n",
    "\n",
    "        if row == 0:\n",
    "            actions.remove(UP)\n",
    "        elif row == nrows-1:\n",
    "            actions.remove(DOWN)\n",
    "\n",
    "        if col == 0:\n",
    "            actions.remove(LEFT)\n",
    "        elif col == ncols-1:\n",
    "            actions.remove(RIGHT)\n",
    "\n",
    "        if row>0 and self.layers[layer][row-1,col] == obstacle_cell_mark:\n",
    "            actions.remove(UP)\n",
    "        if row<nrows-1 and self.layers[layer][row+1,col] == obstacle_cell_mark:\n",
    "            actions.remove(DOWN)\n",
    "\n",
    "        if col>0 and self.layers[layer][row,col-1] == obstacle_cell_mark:\n",
    "            actions.remove(LEFT)\n",
    "        if col<ncols-1 and self.layers[layer][row,col+1] == obstacle_cell_mark:\n",
    "            actions.remove(RIGHT)\n",
    "        \n",
    "        if (row, col) not in self.via_holes:\n",
    "            actions.remove(VIA)\n",
    "        \n",
    "        if self.via_done:\n",
    "            actions.remove(VIA)\n",
    "\n",
    "        return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07d300f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render(env, layer):\n",
    "    plt.grid('on')\n",
    "    nrows, ncols = env.height, env.width\n",
    "    ax = plt.gca()\n",
    "    ax.set_xticks(np.arange(0.5, nrows, 1))\n",
    "    ax.set_yticks(np.arange(0.5, ncols, 1))\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    \n",
    "    canvas = np.copy(env.layers[layer])\n",
    "    \n",
    "    cmap = plt.cm.colors.ListedColormap(['black', 'green', 'blue', 'skyblue', 'gray'])\n",
    "    bounds = [0.0, 0.299, 0.5, 0.799, 0.801, 1.0]\n",
    "    #bounds=[0.0, 0.3, 0.5, 0.8, 1.0]\n",
    "    norm = plt.cm.colors.BoundaryNorm(bounds, cmap.N)\n",
    "    img = plt.imshow(canvas, cmap=cmap, norm=norm, interpolation='none')\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec65e89a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "board = Environment()\n",
    "board.act(DOWN)\n",
    "board.act(DOWN)\n",
    "board.act(RIGHT)\n",
    "board.act(RIGHT)\n",
    "board.act(VIA)\n",
    "board.act(RIGHT)\n",
    "board.act(DOWN)\n",
    "board.act(DOWN)\n",
    "board.act(DOWN)\n",
    "board.act(LEFT)\n",
    "board.act(LEFT)\n",
    "board.act(VIA)\n",
    "board.act(LEFT)\n",
    "front_layer = render(board, \"front\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b043793",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOsAAADrCAYAAACICmHVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAFGUlEQVR4nO3cMWocBxiG4X9FzCIhhwWlcIQLJ5B1a8gBkiZt+lzAOoB0AKVXJ1JIF/ANXPkEIQS12coBsVGRQhDF9sbgSZciOKNoZxjPJz0PuFrx7SJ4YWTYf9I0TQHjt/GhPwDw/4gVQogVQogVQogVQogVQnx0kx/e2tpqZrNZ5zedTqe1Wq3shOz0uWWn3eXlZb169WryvtduFOtsNqu9vb3OH2g+n9disbATstPnlp12Jycn//max2AIIVYIIVYIIVYIIVYIIVYIIVYIIVYIIVYIIVYIIVYIIVYIIVYIIVYIIVYIIVYIMbnuyPdkMnlaVU+rqnZ2dr48Pj7u/KZj+3a+neG27LTb39+v5XK53qWIpmlOq+q0qmp3d7cZ07fq7Qyz0+eWnfV5DIYQYoUQYoUQYoUQYoUQYoUQYoUQYoUQYoUQYoUQYoUQYoUQYoUQYoUQYoUQYoUQLkXYGXTLTjuXIuyMZsvO+jwGQwixQgixQgixQgixQgixQgixQgixQgixQgixQgixQgixQgixQgixQgixQgixQgiXIuwMumWnnUsRdkazZWd9HoMhhFghhFghhFghhFghhFghhFghhFghhFghhFghhFghhFghhFghhFghhFghhFghhEsRPe/8tflx9523r2t1b7OXnTd/XnXeqRrn7/o27rgUMeDOr5896bzz6OKsXj7oZ2ex+LnzTtU4f9e3caeNx2AIIVYIIVYIIVYIIVYIIVYIIVYIIVYIIVYIIVYIIVYIIVYIIVYIIVYIIVYIIVYI4VLESHf6ujjhUkTWjksRgTt9XZxwKSJrp43HYAghVgghVgghVgghVgghVgghVgghVgghVgghVgghVgghVgghVgghVgghVgghVgghVgjhrMtId5x1uZs7zroE7jjrcjd32ngMhhBihRBihRBihRBihRBihRBihRBihRBihRBihRBihRBihRBihRBihRBihRDXfp91zJbLT+vw8LvOO8+ev6jpt9903plcnPW2A/8WfSliY+N+nZ9vd975/Is/anVvs/PO9O3r0e24FJG1c2svRWxvf1UHB1933nn2/EW9fPCk886ji7PR7bgUkbXTxt+sEEKsEEKsEEKsEEKsEEKsEEKsEEKsEEKsEEKsEEKsEEKsEEKsEEKsEEKsECL6UsTu7m91ePh9553mcl6rH3/ovjMf3w63R/SlCDvD7PS5Zafdrb0UYWeYnT637KzP36wQQqwQQqwQQqwQQqwQQqwQQqwQQqwQQqwQQqwQQqwQQqwQQqwQQqwQQqwQIvpSBHfXspZ1WIedd47qqPuHGYhLEXYG3eprZ2O6Ueer8847D6cP693qXecdlyLsjGKnz62+drbn23WwOOi8czQ/qqvFVecdlyKAf4gVQogVQogVQogVQogVQogVQogVQogVQogVQogVQogVQogVQogVQogVQogVQrgUYWfQLTvtXIqwM5otO+vzGAwhxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohXIqwM+iWnXYuRdgZzZad9XkMhhBihRBihRBihRBihRBihRBihRBihRBihRBihRBihRBihRBihRBihRBihRBihRA3uhRRVY+r6pce3veTqvrdTsxOn1t22j1umub+e19pmmbwf1X1k52cnTF+pru44zEYQogVQnyoWE/tRO30uWVnzZ1r/4MJGAePwRBCrBBCrBBCrBBCrBDib7vszIW8LG24AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "back_layer = render(board, \"back\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "857bf9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(model, env):\n",
    "    env.reset()\n",
    "    envstate = env.observe()\n",
    "    while True:\n",
    "        prev_envstate = envstate\n",
    "        # get next action\n",
    "        q = model.predict(prev_envstate)\n",
    "        action = np.argmax(q[0])\n",
    "\n",
    "        # apply action, get rewards and new state\n",
    "        envstate, reward, game_status = env.act(action)\n",
    "        if game_status == 'win':\n",
    "            return True\n",
    "        elif game_status == 'lose':\n",
    "            return False\n",
    "\n",
    "def completion_check(env):\n",
    "    if not env.get_valid_actions(env.start):\n",
    "        return False\n",
    "    if not env.get_valid_actions(env.destination):\n",
    "        return False\n",
    "    return True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4546d33d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion_check(board)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60f2f72",
   "metadata": {},
   "source": [
    "episode = [envstate, action, reward, envstate_next, game_over]\n",
    "\n",
    "    - envstate : environment state 임. 모든 cell 의 상태를 의미. neural network 를 더 쉽게 하기 위해서, grid 를 1차원 vector 로 넣을 것\n",
    "    - action : 4개의 action 중 하나 (0,1,2,3)\n",
    "    - reward : action 에서 얻은 reward\n",
    "    - envstate_next : last action 에 의해 transition 된 grid environment\n",
    "    - game_over : True 또는 False. game 이 끝났는지를 가리킴. Agent 가 destination cell 에 도달하거나, negative reward limit 에 도달하면 끝남. 각각 win 과 lose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25a9df68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory for Episodes\n",
    "class Experience(object):\n",
    "    def __init__(self, model, max_memory=100, discount=0.95):\n",
    "        self.model = model\n",
    "        self.max_memory = max_memory\n",
    "        self.discount = discount\n",
    "        self.memory = list()\n",
    "        self.num_actions = model.output_shape[-1]\n",
    "\n",
    "    def remember(self, episode):\n",
    "        # episode = [envstate, action, reward, envstate_next, game_over]\n",
    "        # memory[i] = episode\n",
    "        # envstate == flattened 1d grid cells info, including agent cell (see method: observe)\n",
    "        self.memory.append(episode)\n",
    "        if len(self.memory) > self.max_memory:\n",
    "            del self.memory[0]\n",
    "\n",
    "    def predict(self, envstate):\n",
    "        return self.model.predict(envstate)[0]\n",
    "\n",
    "    def get_data(self, data_size=10):\n",
    "        env_size = self.memory[0][0].shape[1]   # envstate 1d size (1st element of episode), 첫번째 episode 의 첫번째 element(envstate[1,64])\n",
    "        mem_size = len(self.memory)\n",
    "        data_size = min(mem_size, data_size)\n",
    "        inputs = np.zeros((data_size, env_size))\n",
    "        targets = np.zeros((data_size, self.num_actions))\n",
    "        for i, j in enumerate(np.random.choice(range(mem_size), data_size, replace=False)):\n",
    "            envstate, action, reward, envstate_next, game_over = self.memory[j]\n",
    "            inputs[i] = envstate\n",
    "            # There should be no target values for actions not taken.\n",
    "            targets[i] = self.predict(envstate)\n",
    "            # Q_sa = derived policy = max quality env/action = max_a' Q(s', a')\n",
    "            Q_sa = np.max(self.predict(envstate_next))\n",
    "            if game_over:\n",
    "                targets[i, action] = reward\n",
    "            else:\n",
    "                # reward + gamma * max_a' Q(s', a')\n",
    "                targets[i, action] = reward + self.discount * Q_sa\n",
    "        return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0de56ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(env, lr=0.001):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(env.size, input_shape=(env.size,)))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dense(env.size))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dense(ACTION_NUMS))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3744c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qtrain(model, epochs, max_memory, data_size, weights_file = \"\", name = \"\", environment = None, target_agent_num = 0):\n",
    "    global epsilon\n",
    "    \n",
    "    n_epoch = epochs\n",
    "    max_memory = max_memory\n",
    "    data_size = data_size\n",
    "    weights_file = weights_file\n",
    "    name = name\n",
    "    start_time = datetime.datetime.now()\n",
    "\n",
    "    # If wanna continue training from a previous model.. load the weights_file\n",
    "    if weights_file:\n",
    "        print(\"loading weights from file: %s\" % (weights_file,))\n",
    "        model.load_weights(weights_file)\n",
    "\n",
    "    # Construct environment\n",
    "    env = environment\n",
    "    if not env:\n",
    "        env = Environment()\n",
    "\n",
    "    # Initialize experience replay object\n",
    "    experience = Experience(model, max_memory=max_memory)\n",
    "\n",
    "    win_history = []   # history of win/lose game\n",
    "    hsize = env.size//2   # history window size\n",
    "    win_rate = 0.0\n",
    "\n",
    "    for epoch in range(n_epoch):\n",
    "        loss = 0.0\n",
    "        env.reset(target_agent_num)\n",
    "        while not completion_check(env):\n",
    "            env.reset(target_agent_num)\n",
    "        \n",
    "        game_over = False\n",
    "\n",
    "        # get initial envstate (1d flattened canvas)\n",
    "        envstate = env.observe()\n",
    "\n",
    "        n_episodes = 0\n",
    "        while not game_over:\n",
    "            valid_actions = env.get_valid_actions()\n",
    "            if not valid_actions: break\n",
    "            prev_envstate = envstate\n",
    "            # Get next action\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = random.choice(valid_actions)\n",
    "            else:\n",
    "                action = np.argmax(experience.predict(prev_envstate))\n",
    "\n",
    "            # Apply action, get reward and new envstate\n",
    "            envstate, reward, game_status = env.act(action)\n",
    "            if game_status == 'win':\n",
    "                win_history.append(1)\n",
    "                game_over = True\n",
    "            elif game_status == 'lose':\n",
    "                win_history.append(0)\n",
    "                game_over = True\n",
    "            else:\n",
    "                game_over = False\n",
    "\n",
    "            # Store episode (experience)\n",
    "            episode = [prev_envstate, action, reward, envstate, game_over]\n",
    "            experience.remember(episode)\n",
    "            n_episodes += 1\n",
    "\n",
    "            # Train neural network model\n",
    "            inputs, targets = experience.get_data(data_size=data_size)\n",
    "            h = model.fit(\n",
    "                inputs,\n",
    "                targets,\n",
    "                nb_epoch=8,\n",
    "                batch_size=16,\n",
    "                verbose=0,\n",
    "            )\n",
    "            loss = model.evaluate(inputs, targets, verbose=0)\n",
    "\n",
    "        if len(win_history) > hsize:\n",
    "            win_rate = sum(win_history[-hsize:]) / hsize\n",
    "    \n",
    "        dt = datetime.datetime.now() - start_time\n",
    "        t = format_time(dt.total_seconds())\n",
    "        template = \"Epoch: {:03d}/{:d} | Loss: {:.4f} | Episodes: {:d} | Win count: {:d} | Win rate: {:.3f} | time: {}\"\n",
    "        print(template.format(epoch, n_epoch-1, loss, n_episodes, sum(win_history), win_rate, t))\n",
    "        # we simply check if training has exhausted all free cells and if in all\n",
    "        # cases the agent won\n",
    "        if win_rate > 0.9 : epsilon = 0.05\n",
    "        if sum(win_history[-hsize:]) == hsize and completion_check(env):\n",
    "            print(\"Reached 100%% win rate at epoch: %d\" % (epoch,))\n",
    "            break\n",
    "\n",
    "    # Save trained model weights and architecture, this will be used by the visualization code\n",
    "    h5file = name + \".h5\"\n",
    "    json_file = name + \".json\"\n",
    "    model.save_weights(h5file, overwrite=True)\n",
    "    with open(json_file, \"w\") as outfile:\n",
    "        json.dump(model.to_json(), outfile)\n",
    "    end_time = datetime.datetime.now()\n",
    "    dt = datetime.datetime.now() - start_time\n",
    "    seconds = dt.total_seconds()\n",
    "    t = format_time(seconds)\n",
    "    print('files: %s, %s' % (h5file, json_file))\n",
    "    print(\"n_epoch: %d, max_mem: %d, data: %d, time: %s\" % (epoch, max_memory, data_size, t))\n",
    "    return \n",
    "\n",
    "# This is a small utility for printing readable time strings:\n",
    "def format_time(seconds):\n",
    "    if seconds < 400:\n",
    "        s = float(seconds)\n",
    "        return \"%.1f seconds\" % (s,)\n",
    "    elif seconds < 4000:\n",
    "        m = seconds / 60.0\n",
    "        return \"%.2f minutes\" % (m,)\n",
    "    else:\n",
    "        h = seconds / 3600.0\n",
    "        return \"%.2f hours\" % (h,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2224627",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-d416974cf89c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtemp_env\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEnvironment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_env\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mqtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtemp_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"agent1_weight\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menvironment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtemp_env\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-df06614a8612>\u001b[0m in \u001b[0;36mqtrain\u001b[0;34m(model, epochs, max_memory, data_size, weights_file, name, environment, target_agent_num)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0;31m# Train neural network model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexperience\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m             h = model.fit(\n\u001b[1;32m     69\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-9238bc5053aa>\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(self, data_size)\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menvstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;31m# There should be no target values for actions not taken.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0mtargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menvstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0;31m# Q_sa = derived policy = max quality env/action = max_a' Q(s', a')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mQ_sa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menvstate_next\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-9238bc5053aa>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, envstate)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menvstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menvstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/jjs2/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose)\u001b[0m\n\u001b[1;32m    722\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 724\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    725\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/jjs2/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose)\u001b[0m\n\u001b[1;32m   1270\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m         return self._predict_loop(f, ins,\n\u001b[0;32m-> 1272\u001b[0;31m                                   batch_size=batch_size, verbose=verbose)\n\u001b[0m\u001b[1;32m   1273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m~/anaconda3/envs/jjs2/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_predict_loop\u001b[0;34m(self, f, ins, batch_size, verbose)\u001b[0m\n\u001b[1;32m    933\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m             \u001b[0mprogbar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mProgbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnb_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 935\u001b[0;31m         \u001b[0mbatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    936\u001b[0m         \u001b[0mindex_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    937\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_end\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/jjs2/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mmake_batches\u001b[0;34m(size, batch_size)\u001b[0m\n\u001b[1;32m    273\u001b[0m     \"\"\"Returns a list of batch indices (tuples of indices).\n\u001b[1;32m    274\u001b[0m     \"\"\"\n\u001b[0;32m--> 275\u001b[0;31m     \u001b[0mnb_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m     return [(i * batch_size, min(size, (i + 1) * batch_size))\n\u001b[1;32m    277\u001b[0m             for i in range(0, nb_batch)]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "temp_env = Environment()\n",
    "model1 = build_model(temp_env)\n",
    "qtrain(model1, epochs=1000, max_memory=8*temp_env.size, data_size=32, name=\"agent1_weight\", environment=temp_env)\n",
    "\n",
    "#\n",
    "rendered_front_imgs = []\n",
    "rendered_back_imgs = []\n",
    "\n",
    "temp_env = Environment()\n",
    "envstate = temp_env.observe()\n",
    "count = 0\n",
    "while True:\n",
    "    prev_envstate = envstate\n",
    "    q = model1.predict(prev_envstate)\n",
    "    action = np.argmax(q[0])\n",
    "    envstate, reward, game_status = temp_env.act(action)\n",
    "    rendered_front_imgs.append(render(temp_env, \"front\"))\n",
    "    plt.savefig(\"demo/\"+\"agent1-\"+str(count)+\"-front\"+\".png\")\n",
    "    rendered_back_imgs.append(render(temp_env, \"back\"))\n",
    "    plt.savefig(\"demo/\"+\"agent1-\"+str(count)+\"-back\"+\".png\")\n",
    "    count += 1\n",
    "    if game_status == 'win':\n",
    "        print(\"win\")\n",
    "        break\n",
    "    elif game_status == 'lose':\n",
    "        print(\"lose\")\n",
    "        break\n",
    "\n",
    "# model2 = build_model(temp_env)\n",
    "# qtrain(model2, epochs=1000, max_memory=8*temp_env.size, data_size=32, name=\"agent2_weight\", environment=temp_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3fc198",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7686098e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a3d371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lose\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "footprints = []\n",
    "target_agent_num = 1\n",
    "\n",
    "temp_env = Environment()\n",
    "envstate = temp_env.observe()\n",
    "count = 0\n",
    "while True:\n",
    "    prev_envstate = envstate\n",
    "    q = model1.predict(prev_envstate)\n",
    "    action = np.argmax(q[0])\n",
    "    envstate, reward, game_status = temp_env.act(action)\n",
    "    footprints.append(temp_env.agent)\n",
    "    rendered_front_imgs.append(render(temp_env, \"front\"))\n",
    "    plt.savefig(\"demo/\"+\"agent1-\"+str(count)+\"-front\"+\".png\")\n",
    "    rendered_back_imgs.append(render(temp_env, \"back\"))\n",
    "    plt.savefig(\"demo/\"+\"agent1-\"+str(count)+\"-back\"+\".png\")\n",
    "    count += 1\n",
    "    if game_status == 'win':\n",
    "        print(\"win\")\n",
    "        break\n",
    "    elif game_status == 'lose':\n",
    "        print(\"lose\")\n",
    "        break\n",
    "for footprint in footprints:\n",
    "    if footprint[2] == \"front\":\n",
    "        temp_env.original_front_layer[footprint[0], footprint[1]] = obstacle_cell_mark\n",
    "    elif footprint[2] == \"back\":\n",
    "        temp_env.original_back_layer[footprint[0], footprint[1]] = obstacle_cell_mark\n",
    "\n",
    "temp_env.reset(target_agent_num)\n",
    "front_layer = render(temp_env, \"front\")\n",
    "\n",
    "# model2 = build_model(temp_env)\n",
    "# qtrain(model2, epochs=1000, max_memory=8*temp_env.size, data_size=32, name=\"agent2_weight\", environment=temp_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e96891c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000/999 | Loss: 0.0029 | Episodes: 171 | Win count: 0 | Win rate: 0.000 | time: 15.6 seconds\n",
      "Epoch: 001/999 | Loss: 0.0023 | Episodes: 169 | Win count: 0 | Win rate: 0.000 | time: 32.1 seconds\n",
      "Epoch: 002/999 | Loss: 0.0173 | Episodes: 196 | Win count: 0 | Win rate: 0.000 | time: 51.2 seconds\n",
      "Epoch: 003/999 | Loss: 0.0297 | Episodes: 153 | Win count: 0 | Win rate: 0.000 | time: 66.0 seconds\n",
      "Epoch: 004/999 | Loss: 0.0032 | Episodes: 141 | Win count: 0 | Win rate: 0.000 | time: 79.7 seconds\n",
      "Epoch: 005/999 | Loss: 0.0018 | Episodes: 149 | Win count: 0 | Win rate: 0.000 | time: 94.4 seconds\n",
      "Epoch: 006/999 | Loss: 0.0031 | Episodes: 165 | Win count: 0 | Win rate: 0.000 | time: 110.5 seconds\n",
      "Epoch: 007/999 | Loss: 0.0189 | Episodes: 180 | Win count: 0 | Win rate: 0.000 | time: 128.2 seconds\n",
      "Epoch: 008/999 | Loss: 0.0025 | Episodes: 155 | Win count: 1 | Win rate: 0.000 | time: 143.2 seconds\n",
      "Epoch: 009/999 | Loss: 0.0022 | Episodes: 119 | Win count: 2 | Win rate: 0.000 | time: 154.9 seconds\n",
      "Epoch: 010/999 | Loss: 0.0020 | Episodes: 28 | Win count: 3 | Win rate: 0.000 | time: 157.6 seconds\n",
      "Epoch: 011/999 | Loss: 0.0025 | Episodes: 176 | Win count: 4 | Win rate: 0.000 | time: 174.7 seconds\n",
      "Epoch: 012/999 | Loss: 0.0019 | Episodes: 197 | Win count: 4 | Win rate: 0.000 | time: 194.0 seconds\n",
      "Epoch: 013/999 | Loss: 0.0022 | Episodes: 42 | Win count: 5 | Win rate: 0.000 | time: 198.0 seconds\n",
      "Epoch: 014/999 | Loss: 0.0146 | Episodes: 190 | Win count: 5 | Win rate: 0.000 | time: 216.6 seconds\n",
      "Epoch: 015/999 | Loss: 0.0018 | Episodes: 179 | Win count: 6 | Win rate: 0.000 | time: 233.9 seconds\n",
      "Epoch: 016/999 | Loss: 0.0459 | Episodes: 20 | Win count: 7 | Win rate: 0.000 | time: 235.8 seconds\n",
      "Epoch: 017/999 | Loss: 0.0217 | Episodes: 130 | Win count: 8 | Win rate: 0.000 | time: 248.4 seconds\n",
      "Epoch: 018/999 | Loss: 0.0029 | Episodes: 14 | Win count: 9 | Win rate: 0.000 | time: 249.7 seconds\n",
      "Epoch: 019/999 | Loss: 0.0084 | Episodes: 25 | Win count: 10 | Win rate: 0.000 | time: 252.1 seconds\n",
      "Epoch: 020/999 | Loss: 0.0030 | Episodes: 29 | Win count: 11 | Win rate: 0.000 | time: 255.0 seconds\n",
      "Epoch: 021/999 | Loss: 0.0023 | Episodes: 180 | Win count: 11 | Win rate: 0.000 | time: 272.5 seconds\n",
      "Epoch: 022/999 | Loss: 0.0222 | Episodes: 46 | Win count: 12 | Win rate: 0.000 | time: 276.9 seconds\n",
      "Epoch: 023/999 | Loss: 0.0031 | Episodes: 177 | Win count: 12 | Win rate: 0.000 | time: 294.0 seconds\n",
      "Epoch: 024/999 | Loss: 0.0019 | Episodes: 192 | Win count: 12 | Win rate: 0.000 | time: 312.7 seconds\n",
      "Epoch: 025/999 | Loss: 0.0025 | Episodes: 164 | Win count: 12 | Win rate: 0.000 | time: 328.7 seconds\n",
      "Epoch: 026/999 | Loss: 0.0019 | Episodes: 26 | Win count: 13 | Win rate: 0.000 | time: 331.2 seconds\n",
      "Epoch: 027/999 | Loss: 0.0038 | Episodes: 190 | Win count: 13 | Win rate: 0.000 | time: 349.7 seconds\n",
      "Epoch: 028/999 | Loss: 0.0019 | Episodes: 33 | Win count: 14 | Win rate: 0.000 | time: 352.9 seconds\n",
      "Epoch: 029/999 | Loss: 0.0022 | Episodes: 16 | Win count: 15 | Win rate: 0.000 | time: 354.4 seconds\n",
      "Epoch: 030/999 | Loss: 0.0135 | Episodes: 181 | Win count: 15 | Win rate: 0.000 | time: 372.1 seconds\n",
      "Epoch: 031/999 | Loss: 0.0068 | Episodes: 85 | Win count: 16 | Win rate: 0.000 | time: 380.4 seconds\n",
      "Epoch: 032/999 | Loss: 0.0029 | Episodes: 19 | Win count: 17 | Win rate: 0.000 | time: 382.3 seconds\n",
      "Epoch: 033/999 | Loss: 0.0038 | Episodes: 164 | Win count: 17 | Win rate: 0.000 | time: 398.3 seconds\n",
      "Epoch: 034/999 | Loss: 0.0023 | Episodes: 27 | Win count: 18 | Win rate: 0.000 | time: 6.68 minutes\n",
      "Epoch: 035/999 | Loss: 0.0103 | Episodes: 78 | Win count: 19 | Win rate: 0.000 | time: 6.81 minutes\n",
      "Epoch: 036/999 | Loss: 0.0111 | Episodes: 177 | Win count: 19 | Win rate: 0.000 | time: 7.10 minutes\n",
      "Epoch: 037/999 | Loss: 0.0128 | Episodes: 82 | Win count: 20 | Win rate: 0.000 | time: 7.23 minutes\n",
      "Epoch: 038/999 | Loss: 0.0126 | Episodes: 74 | Win count: 21 | Win rate: 0.000 | time: 7.35 minutes\n",
      "Epoch: 039/999 | Loss: 0.0052 | Episodes: 14 | Win count: 22 | Win rate: 0.000 | time: 7.37 minutes\n",
      "Epoch: 040/999 | Loss: 0.0048 | Episodes: 132 | Win count: 23 | Win rate: 0.000 | time: 7.59 minutes\n",
      "Epoch: 041/999 | Loss: 0.0026 | Episodes: 29 | Win count: 24 | Win rate: 0.000 | time: 7.63 minutes\n",
      "Epoch: 042/999 | Loss: 0.0232 | Episodes: 15 | Win count: 25 | Win rate: 0.000 | time: 7.66 minutes\n",
      "Epoch: 043/999 | Loss: 0.0021 | Episodes: 29 | Win count: 26 | Win rate: 0.000 | time: 7.71 minutes\n",
      "Epoch: 044/999 | Loss: 0.0038 | Episodes: 187 | Win count: 26 | Win rate: 0.000 | time: 8.01 minutes\n",
      "Epoch: 045/999 | Loss: 0.0030 | Episodes: 20 | Win count: 27 | Win rate: 0.000 | time: 8.04 minutes\n",
      "Epoch: 046/999 | Loss: 0.0057 | Episodes: 67 | Win count: 28 | Win rate: 0.000 | time: 8.15 minutes\n",
      "Epoch: 047/999 | Loss: 0.0236 | Episodes: 14 | Win count: 29 | Win rate: 0.000 | time: 8.17 minutes\n",
      "Epoch: 048/999 | Loss: 0.0070 | Episodes: 14 | Win count: 30 | Win rate: 0.000 | time: 8.20 minutes\n",
      "Epoch: 049/999 | Loss: 0.0046 | Episodes: 16 | Win count: 31 | Win rate: 0.000 | time: 8.22 minutes\n",
      "Epoch: 050/999 | Loss: 0.0080 | Episodes: 149 | Win count: 32 | Win rate: 0.000 | time: 8.46 minutes\n",
      "Epoch: 051/999 | Loss: 0.0089 | Episodes: 45 | Win count: 33 | Win rate: 0.000 | time: 8.54 minutes\n",
      "Epoch: 052/999 | Loss: 0.0210 | Episodes: 34 | Win count: 34 | Win rate: 0.000 | time: 8.59 minutes\n",
      "Epoch: 053/999 | Loss: 0.0028 | Episodes: 21 | Win count: 35 | Win rate: 0.000 | time: 8.63 minutes\n",
      "Epoch: 054/999 | Loss: 0.0053 | Episodes: 43 | Win count: 36 | Win rate: 0.000 | time: 8.70 minutes\n",
      "Epoch: 055/999 | Loss: 0.0285 | Episodes: 56 | Win count: 37 | Win rate: 0.000 | time: 8.79 minutes\n",
      "Epoch: 056/999 | Loss: 0.0023 | Episodes: 75 | Win count: 38 | Win rate: 0.000 | time: 8.91 minutes\n",
      "Epoch: 057/999 | Loss: 0.0082 | Episodes: 43 | Win count: 39 | Win rate: 0.000 | time: 8.98 minutes\n",
      "Epoch: 058/999 | Loss: 0.0035 | Episodes: 169 | Win count: 39 | Win rate: 0.000 | time: 9.25 minutes\n",
      "Epoch: 059/999 | Loss: 0.0084 | Episodes: 26 | Win count: 40 | Win rate: 0.000 | time: 9.30 minutes\n",
      "Epoch: 060/999 | Loss: 0.0025 | Episodes: 101 | Win count: 41 | Win rate: 0.000 | time: 9.46 minutes\n",
      "Epoch: 061/999 | Loss: 0.0024 | Episodes: 23 | Win count: 42 | Win rate: 0.000 | time: 9.50 minutes\n",
      "Epoch: 062/999 | Loss: 0.0060 | Episodes: 27 | Win count: 43 | Win rate: 0.000 | time: 9.54 minutes\n",
      "Epoch: 063/999 | Loss: 0.0075 | Episodes: 179 | Win count: 43 | Win rate: 0.000 | time: 9.83 minutes\n",
      "Epoch: 064/999 | Loss: 0.0086 | Episodes: 14 | Win count: 44 | Win rate: 0.000 | time: 9.85 minutes\n",
      "Epoch: 065/999 | Loss: 0.0046 | Episodes: 74 | Win count: 45 | Win rate: 0.000 | time: 9.97 minutes\n",
      "Epoch: 066/999 | Loss: 0.0103 | Episodes: 52 | Win count: 46 | Win rate: 0.000 | time: 10.06 minutes\n",
      "Epoch: 067/999 | Loss: 0.0021 | Episodes: 22 | Win count: 47 | Win rate: 0.000 | time: 10.10 minutes\n",
      "Epoch: 068/999 | Loss: 0.0075 | Episodes: 33 | Win count: 48 | Win rate: 0.000 | time: 10.15 minutes\n",
      "Epoch: 069/999 | Loss: 0.0048 | Episodes: 128 | Win count: 49 | Win rate: 0.000 | time: 10.36 minutes\n",
      "Epoch: 070/999 | Loss: 0.0041 | Episodes: 54 | Win count: 50 | Win rate: 0.000 | time: 10.45 minutes\n",
      "Epoch: 071/999 | Loss: 0.0057 | Episodes: 14 | Win count: 51 | Win rate: 0.000 | time: 10.47 minutes\n",
      "Epoch: 072/999 | Loss: 0.0184 | Episodes: 176 | Win count: 51 | Win rate: 0.000 | time: 10.75 minutes\n",
      "Epoch: 073/999 | Loss: 0.0046 | Episodes: 109 | Win count: 52 | Win rate: 0.000 | time: 10.93 minutes\n",
      "Epoch: 074/999 | Loss: 0.0057 | Episodes: 77 | Win count: 53 | Win rate: 0.000 | time: 11.05 minutes\n",
      "Epoch: 075/999 | Loss: 0.0038 | Episodes: 24 | Win count: 54 | Win rate: 0.000 | time: 11.09 minutes\n",
      "Epoch: 076/999 | Loss: 0.0273 | Episodes: 35 | Win count: 55 | Win rate: 0.000 | time: 11.15 minutes\n",
      "Epoch: 077/999 | Loss: 0.0036 | Episodes: 35 | Win count: 56 | Win rate: 0.000 | time: 11.21 minutes\n",
      "Epoch: 078/999 | Loss: 0.0088 | Episodes: 29 | Win count: 57 | Win rate: 0.000 | time: 11.25 minutes\n",
      "Epoch: 079/999 | Loss: 0.0117 | Episodes: 47 | Win count: 58 | Win rate: 0.000 | time: 11.33 minutes\n",
      "Epoch: 080/999 | Loss: 0.0040 | Episodes: 16 | Win count: 59 | Win rate: 0.000 | time: 11.36 minutes\n",
      "Epoch: 081/999 | Loss: 0.0042 | Episodes: 80 | Win count: 60 | Win rate: 0.000 | time: 11.49 minutes\n",
      "Epoch: 082/999 | Loss: 0.0132 | Episodes: 30 | Win count: 61 | Win rate: 0.000 | time: 11.54 minutes\n",
      "Epoch: 083/999 | Loss: 0.0071 | Episodes: 14 | Win count: 62 | Win rate: 0.000 | time: 11.56 minutes\n",
      "Epoch: 084/999 | Loss: 0.0144 | Episodes: 23 | Win count: 63 | Win rate: 0.000 | time: 11.59 minutes\n",
      "Epoch: 085/999 | Loss: 0.0031 | Episodes: 103 | Win count: 64 | Win rate: 0.000 | time: 11.76 minutes\n",
      "Epoch: 086/999 | Loss: 0.0089 | Episodes: 39 | Win count: 65 | Win rate: 0.000 | time: 11.82 minutes\n",
      "Epoch: 087/999 | Loss: 0.0075 | Episodes: 14 | Win count: 66 | Win rate: 0.000 | time: 11.85 minutes\n",
      "Epoch: 088/999 | Loss: 0.0092 | Episodes: 187 | Win count: 66 | Win rate: 0.000 | time: 12.15 minutes\n",
      "Epoch: 089/999 | Loss: 0.0032 | Episodes: 59 | Win count: 67 | Win rate: 0.000 | time: 12.25 minutes\n",
      "Epoch: 090/999 | Loss: 0.0084 | Episodes: 26 | Win count: 68 | Win rate: 0.000 | time: 12.29 minutes\n",
      "Epoch: 091/999 | Loss: 0.0104 | Episodes: 51 | Win count: 69 | Win rate: 0.000 | time: 12.37 minutes\n",
      "Epoch: 092/999 | Loss: 0.0098 | Episodes: 19 | Win count: 70 | Win rate: 0.000 | time: 12.40 minutes\n",
      "Epoch: 093/999 | Loss: 0.0127 | Episodes: 14 | Win count: 71 | Win rate: 0.000 | time: 12.42 minutes\n",
      "Epoch: 094/999 | Loss: 0.0039 | Episodes: 16 | Win count: 72 | Win rate: 0.000 | time: 12.45 minutes\n"
     ]
    }
   ],
   "source": [
    "model2 = build_model(temp_env)\n",
    "qtrain(model2, epochs=1000, max_memory=8*temp_env.size, data_size=32, name=\"agent2_weight\", environment=temp_env, target_agent_num=target_agent_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabc7c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "win\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOsAAADrCAYAAACICmHVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAEDElEQVR4nO3cwUlcURiG4TMhIAiBAbOyAFNAeooFWIorrSkNxAJcZeFKcHXTQHKN1+HmvOR5ICvlm9m8cAj4H5ZlGcD8PvzrLwD8HbFChFghQqwQIVaIECtEfHzLL5+fny/H4/HdH3p2djZeXl7sRHZOuWVn3dPT03h+fj787mdvivV4PI7r6+t3f6Grq6vx8PBgJ7Jzyi076+7u7v74M89giBArRIgVIsQKEWKFCLFChFghQqwQIVaIECtEiBUixAoRYoUIsUKEWCFCrBBxeO3I9+Fw+DbG+DbGGBcXF19vb2/f/aGz/XW+nf227Ky7ubkZj4+P2y5FLMtyP8a4H2OMy8vLZaa/qrezz84pt+xs5xkMEWKFCLFChFghQqwQIVaIECtEiBUixAoRYoUIsUKEWCFCrBAhVogQK0SIFSJcirCz65addS5F2Jlmy852nsEQIVaIECtEiBUixAoRYoUIsUKEWCFCrBAhVogQK0SIFSLEChFihQixQoRYIcKlCDu7btlZ51KEnWm27GznGQwRYoUIsUKEWCFCrBAhVogQK0SIFSLEChFihQixQoRYIUKsECFWiBArRIgVIlyKsLPrlp11LkXYmWbLznaewRAhVogQK0SIFSLEChFihQixQoRYIUKsECFWiBArRIgVIsQKEWKFCLFChFghwqUIO7tu2VnnUoSdabbsbOcZDBFihQixQoRYIUKsECFWiBArRIgVIsQKEWKFCLFChFghQqwQIVaIECtEiBUixAoRzrrY2XXLzjpnXexMs2VnO89giBArRIgVIsQKEWKFCLFChFghQqwQIVaIECtEiBUixAoRYoUIsUKEWCFCrBDhUoSdXbfsrHMpws40W3a28wyGCLFChFghQqwQIVaIECtEiBUixAoRYoUIsUKEWCFCrBAhVogQK0SIFSLEChEuRdjZdcvOOpci7EyzZWc7z2CIECtEiBUixAoRYoUIsUKEWCFCrBAhVogQK0SIFSLEChFihQixQoRYIUKsEOFShJ1dt+yscynCzjRbdrbzDIYIsUKEWCFCrBAhVogQK0SIFSLEChFihQixQoRYIUKsECFWiBArRIgVIsQKES5F2Nl1y846lyLsTLNlZzvPYIgQK0SIFSLEChFihQixQoRYIUKsECFWiBArRIgVIsQKEWKFCLFChFghQqwQ4VKEnV237KxzKcLONFt2tvMMhgixQoRYIUKsECFWiBArRIgVIsQKEWKFCLFChFghQqwQIVaIECtEiBUixAoRb7oUMcb4Msb4cYLP/TzG+Gkns3PKLTvrvizL8um3P1mWZfd/Y4zvdjo7M36n/3HHMxgixAoR/yrWezupnVNu2dm48+p/MAFz8AyGCLFChFghQqwQIVaI+AU4tq7wHdU3XQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "footprints2 = []\n",
    "target_agent_num = 1\n",
    "\n",
    "temp_env.reset(target_agent_num)\n",
    "envstate = temp_env.observe()\n",
    "count = 0\n",
    "while True:\n",
    "    prev_envstate = envstate\n",
    "    q = model2.predict(prev_envstate)\n",
    "    action = np.argmax(q[0])\n",
    "    envstate, reward, game_status = temp_env.act(action)\n",
    "    footprints.append(temp_env.agent)\n",
    "    rendered_front_imgs.append(render(temp_env, \"front\"))\n",
    "    plt.savefig(\"demo/\"+\"agent2-\"+str(count)+\"-front\"+\".png\")\n",
    "    rendered_back_imgs.append(render(temp_env, \"back\"))\n",
    "    plt.savefig(\"demo/\"+\"agent2-\"+str(count)+\"-back\"+\".png\")\n",
    "    count += 1\n",
    "    if game_status == 'win':\n",
    "        print(\"win\")\n",
    "        break\n",
    "    elif game_status == 'lose':\n",
    "        print(\"lose\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f7b969",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561e7380",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate(model, epoch = 2):\n",
    "    for i in range(epoch):\n",
    "        rendered_imgs = []\n",
    "        count = 0\n",
    "        \n",
    "        env = Environment()\n",
    "        while not completion_check(env):\n",
    "            env = Environment()\n",
    "        envstate = env.observe()\n",
    "        rendered_imgs.append(render(env))\n",
    "        plt.savefig(\"demo/\"+str(i)+\"-\"+str(count)+\".png\")\n",
    "        count += 1\n",
    "        while True:\n",
    "            prev_envstate = envstate\n",
    "            q = model.predict(prev_envstate)\n",
    "            action = np.argmax(q[0])\n",
    "            envstate, reward, game_status = env.act(action)\n",
    "            rendered_imgs.append(render(env))\n",
    "            plt.savefig(\"demo/\"+str(i)+\"-\"+str(count)+\".png\")\n",
    "            count += 1\n",
    "            if game_status == 'win':\n",
    "                print(\"win\")\n",
    "                break\n",
    "            elif game_status == 'lose':\n",
    "                print(\"lose\")\n",
    "                break\n",
    "        print(len(rendered_imgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd65b11f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jjs2",
   "language": "python",
   "name": "jjs2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a27823e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/sjy/anaconda3/envs/jjs2/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:463: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/sjy/anaconda3/envs/jjs2/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:464: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/sjy/anaconda3/envs/jjs2/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:465: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/sjy/anaconda3/envs/jjs2/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:466: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/sjy/anaconda3/envs/jjs2/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:467: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n"
     ]
    }
   ],
   "source": [
    "# import requirements\n",
    "from __future__ import print_function\n",
    "import os, sys, time, datetime, json, random\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD , Adam, RMSprop\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab27dc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "### main constants\n",
    "\n",
    "# markers\n",
    "free_cell_mark = 1.0\n",
    "visited_cell_mark = 0.8 \n",
    "agent_cell_mark = 0.5  \n",
    "obstacle_cell_mark = 0.0\n",
    "destination_cell_mark = 0.3\n",
    "\n",
    "# rewards\n",
    "GOAL = 1.0\n",
    "REVISIT = -0.25\n",
    "VALID = -0.04\n",
    "INVALID = -0.8\n",
    "# BLOCKED = -0.75\n",
    "\n",
    "\n",
    "# Actions \n",
    "LEFT = 0\n",
    "UP = 1\n",
    "RIGHT = 2\n",
    "DOWN = 3\n",
    "actions_dict = {\n",
    "    LEFT: 'left',\n",
    "    UP: 'up',\n",
    "    RIGHT: 'right',\n",
    "    DOWN: 'down',\n",
    "}\n",
    "\n",
    "ACTION_NUMS = len(actions_dict)\n",
    "\n",
    "# Exploration factor\n",
    "epsilon = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4299cb4",
   "metadata": {},
   "source": [
    "- free cell : 1.0\n",
    "- visited cell : 0.8\n",
    "- agent cell : 0.5\n",
    "- occupied cell : 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a55aeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_obstacles =  np.array([\n",
    "    [ 1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  0.,  0.,  0.,  0.,  0.,  1.],\n",
    "    [ 1.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  0.,  1.,  1.,  0.,  0.,  0.],\n",
    "    [ 1.,  0.,  0.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  1.,  1.,  1.,  1.]\n",
    "])\n",
    "\n",
    "# Grid Environment\n",
    "class Environment(object):\n",
    "    def __init__(self, grid_heigth=7, grid_width=7, num_obstacles=7):\n",
    "        self.height = grid_heigth\n",
    "        self.width = grid_width\n",
    "        self.grid = np.ones([self.height, self.width])\n",
    "        self.free_cells = [(r,c) for r in range(self.grid.shape[0]) for c in range(self.grid.shape[1]) if self.grid[r,c] == 1.0]\n",
    "        self.obstacle_cells = []\n",
    "        \n",
    "        self.reset(num_obstacles)\n",
    "\n",
    "    def reset(self, num_obstacles=7):\n",
    "        self.grid = np.ones([self.height, self.width])\n",
    "        self.free_cells = [(r,c) for r in range(self.grid.shape[0]) for c in range(self.grid.shape[1]) if self.grid[r,c] == 1.0]\n",
    "        self.obstacle_cells = []\n",
    "        self.start = (np.random.randint(self.grid.shape[0]), np.random.randint(self.grid.shape[1]))\n",
    "        self.start = (0,0) # TODO: to be removed - it's for fixed start location, not randomly generated\n",
    "        self.agent = np.array(self.start)\n",
    "        if self.start in self.free_cells:\n",
    "            self.free_cells.remove(self.start)\n",
    "        self.grid[self.agent[0], self.agent[1]] = agent_cell_mark\n",
    "        self.destination = (np.random.randint(self.grid.shape[0]), np.random.randint(self.grid.shape[1]))\n",
    "        self.destination = (self.grid.shape[0]-1,self.grid.shape[1]-1) # TODO: to be removed - it's for fixed start location, not randomly generated\n",
    "        while np.array_equal(self.start, self.destination):\n",
    "            self.destination = (np.random.randint(self.grid.shape[0]), np.random.randint(self.grid.shape[1]))\n",
    "        self.grid[self.destination[0], self.destination[1]] = destination_cell_mark\n",
    "        \n",
    "        # TODO: to be uncommented - randomly genererate the obstacles\n",
    "#         for _ in range(num_obstacles):\n",
    "#             x, y = np.random.randint(self.grid.shape[0]), np.random.randint(self.grid.shape[1])\n",
    "#             if (x == self.start[0] and y == self.start[1]) or (x == self.destination[0] and y == self.destination[1]):\n",
    "#                 continue\n",
    "#             self.grid[x, y] = obstacle_cell_mark\n",
    "#             if (x,y) in self.free_cells:\n",
    "#                 self.free_cells.remove((x,y))\n",
    "#             self.obstacle_cells.append((x,y))\n",
    "        \n",
    "        # TODO: to be removed - it's for fixed start location, not randomly generated\n",
    "        for x in range(fixed_obstacles.shape[0]):\n",
    "            for y in range(fixed_obstacles.shape[1]):\n",
    "                if fixed_obstacles[x, y] == obstacle_cell_mark:\n",
    "                    self.grid[x, y] = obstacle_cell_mark\n",
    "                    if (x,y) in self.free_cells:\n",
    "                        self.free_cells.remove((x,y))\n",
    "                    self.obstacle_cells.append((x,y))\n",
    "        \n",
    "        self.min_reward = -0.5 * self.grid.size\n",
    "        self.agent_state = (self.agent[0], self.agent[1], 'start')\n",
    "        \n",
    "        self.total_reward = 0.0\n",
    "        \n",
    "        self.visited = set()\n",
    "        \n",
    "\n",
    "    def update_state(self, action):\n",
    "        nrows, ncols = self.grid.shape\n",
    "        nrow, ncol, nmode = agent_row, agent_col, mode = self.agent_state\n",
    "\n",
    "        if self.grid[agent_row, agent_col] > 0.0:\n",
    "            self.visited.add((agent_row, agent_col))  # mark visited cell\n",
    "\n",
    "        valid_actions = self.get_valid_actions()\n",
    "                \n",
    "        if not valid_actions:\n",
    "            nmode = 'blocked'\n",
    "        elif action in valid_actions:\n",
    "            nmode = 'valid'\n",
    "            if action == LEFT:\n",
    "                ncol -= 1\n",
    "            elif action == UP:\n",
    "                nrow -= 1\n",
    "            if action == RIGHT:\n",
    "                ncol += 1\n",
    "            elif action == DOWN:\n",
    "                nrow += 1\n",
    "        else:                  # invalid action, no change in agent position\n",
    "            mode = 'invalid'\n",
    "\n",
    "        # new state\n",
    "        self.agent_state = (nrow, ncol, nmode)\n",
    "        self.grid[self.agent[0], self.agent[1]] = visited_cell_mark\n",
    "        self.agent = (nrow, ncol)\n",
    "        self.grid[self.agent[0], self.agent[1]] = agent_cell_mark\n",
    "\n",
    "    def get_reward(self):\n",
    "        agent_row, agent_col, mode = self.agent_state\n",
    "        nrows, ncols = self.grid.shape\n",
    "        if agent_row == self.destination[0] and agent_col == self.destination[1]:\n",
    "            return GOAL\n",
    "        if mode == 'blocked':\n",
    "            return self.min_reward - 1 ###################????\n",
    "        if (agent_row, agent_col) in self.visited:\n",
    "            return REVISIT\n",
    "        if mode == 'invalid':\n",
    "            return INVALID\n",
    "        if mode == 'valid':\n",
    "            return VALID\n",
    "\n",
    "    def act(self, action):\n",
    "        self.update_state(action)\n",
    "        reward = self.get_reward()\n",
    "        self.total_reward += reward\n",
    "        status = self.game_status()\n",
    "        envstate = self.observe()\n",
    "        return envstate, reward, status\n",
    "\n",
    "    def observe(self):\n",
    "        canvas = self.draw_env()\n",
    "        envstate = canvas.reshape((1, -1))\n",
    "        return envstate\n",
    "\n",
    "    def draw_env(self):\n",
    "        canvas = np.copy(self.grid)\n",
    "        return canvas\n",
    "\n",
    "    def game_status(self):\n",
    "        if self.total_reward < self.min_reward:\n",
    "            return 'lose'\n",
    "        agent_row, agent_col, mode = self.agent_state\n",
    "        nrows, ncols = self.grid.shape\n",
    "        if agent_row == self.destination[0] and agent_col == self.destination[1]:\n",
    "            return 'win'\n",
    "\n",
    "        return 'not_over'\n",
    "\n",
    "    def get_valid_actions(self, cell=None):\n",
    "        if cell is None:\n",
    "            row, col, mode = self.agent_state\n",
    "        else:\n",
    "            row, col = cell\n",
    "        actions = [LEFT, UP, RIGHT, DOWN]\n",
    "        nrows, ncols = self.grid.shape\n",
    "        if row == 0:\n",
    "            actions.remove(UP)\n",
    "        elif row == nrows-1:\n",
    "            actions.remove(DOWN)\n",
    "\n",
    "        if col == 0:\n",
    "            actions.remove(LEFT)\n",
    "        elif col == ncols-1:\n",
    "            actions.remove(RIGHT)\n",
    "\n",
    "        if row>0 and self.grid[row-1,col] == obstacle_cell_mark:\n",
    "            actions.remove(UP)\n",
    "        if row<nrows-1 and self.grid[row+1,col] == obstacle_cell_mark:\n",
    "            actions.remove(DOWN)\n",
    "\n",
    "        if col>0 and self.grid[row,col-1] == obstacle_cell_mark:\n",
    "            actions.remove(LEFT)\n",
    "        if col<ncols-1 and self.grid[row,col+1] == obstacle_cell_mark:\n",
    "            actions.remove(RIGHT)\n",
    "\n",
    "        return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07d300f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render(env):\n",
    "    plt.grid('on')\n",
    "    nrows, ncols = env.grid.shape\n",
    "    ax = plt.gca()\n",
    "    ax.set_xticks(np.arange(0.5, nrows, 1))\n",
    "    ax.set_yticks(np.arange(0.5, ncols, 1))\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    \n",
    "    canvas = np.copy(env.grid)\n",
    "    \n",
    "    cmap = plt.cm.colors.ListedColormap(['black', 'green', 'blue', 'skyblue', 'gray'])\n",
    "    bounds = [0.0, 0.299, 0.5, 0.799, 0.801, 1.0]\n",
    "    #bounds=[0.0, 0.3, 0.5, 0.8, 1.0]\n",
    "    norm = plt.cm.colors.BoundaryNorm(bounds, cmap.N)\n",
    "    img = plt.imshow(canvas, cmap=cmap, norm=norm, interpolation='none')\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec65e89a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fda55c14cf8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOsAAADrCAYAAACICmHVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAE+ElEQVR4nO3dMW4UdxjG4W9GkaZxKtOwcmGa5QDkNBwAH2B9ANJQbecKHwAuwwXihsZaN4lEgZC24Z+CKmDv2knsv1/meUrW4h2EfjBbfUNrrYDHb+z9AMDtiBVCiBVCiBVCiBVCiBVC/HKXHx6GJ63q+J4eZbfj479qu9122Z6mybbtB/Hp06f68uXLcN1nd4r1W6gf/vsT/Qtv3ryvi4uLLtvL5dK27Qfx9u3bGz/zGgwhxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAoh7nSY6unTTZ2c/H5fz7LHstMuPA57Yx2G4VVVvaqqOjw8rOWyTzTTNNm2/dNv77I31tbaeVWdV1UtFos2xzN8tm0/Br6zQgixQgixQgixQgixQgixQgixQgixQgixQgixQgixQgixQgixQgixQgixQgixQgixQgixQgixQgixQgixQgixQgixQgixQgixQgixQgixQgixQog7nXzsabPZ1OvXr7tsr9dr2zPbfoyG1truH/jnyccXZ2dnD/FcPxjHsS4vL7tsHx0d2Z7Z9tevX7tsr1ar2mw2w3WfxZx8PDg4qNPT0y7b6/Xa9sy2P3/+3GV7F99ZIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIUTMyceeFotFt/ODtvts9zrAtsveWL87+VjL5fLeH+o64zh2u5s5TVO3P7fteW3v4uTjLbx7967bv7TL5dL2jLZ38Z0VQogVQogVQogVQogVQogVQogVQogVQogVQogVQogVQogVQogVQogVQogVQogVQogVQogVQogVQogVQogVQogVQogVQogVQogVQogVQogVQogVQogVQjj5eAtzPT84TVMdHBx02R7HcZbbV1dXN342tNZu/RstFot2cnLyfzzTnTn52Gf75cuXXbbX63W3v++e21VVrbXhul/3GgwhxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAoh9l6R++7k44uzs7OHeK4fjONYl5eXXbaPjo66bT979qy2222X7WmabD+w1WpVm83m2itye++zttbOq+q86tvJx17nB3uefOx5AnDO5ybnuL2L12AIIVYIIVYIIVYIIVYIIVYIIVYIIVYIIVYIIVYIIVYIIVYIIVYIIVYIIVYIIVYIIVYIIVYIIVYIIVYIIVYIIVYIIVYIIVYIIVYIIVYIIVYIIVYIIVYIEXPyca4nAG332f64/dhl+3R1Wm3Tsk8+zvUEoO0+26cXfU587uI1GEKIFUKIFUKIFUKIFUKIFUKIFUKIFUKIFUKIFUKIFUKIFUKIFUKIFUKIFUKIFUKIFUKIFUKIFUKIFUKIFUKIFUKIFUKIFUKIFUKIFUKIFUKIFUKIFULc6eRjVT2vqj/u+6Fu8KSq/rRt+yffft5a+/W6D/bG+lgMw/Chtfabbdtz3fYaDCHECiGSYj23bXvO2zHfWWHukv5nhVkTK4QQK4QQK4QQK4T4G3mGiGRQ5PKsAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "grid_world = Environment()\n",
    "render(grid_world)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ab76d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5 1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  0.  0.  0.  0.  0.  1. ]\n",
      " [1.  0.  1.  1.  1.  1.  1. ]\n",
      " [1.  0.  1.  1.  0.  0.  0. ]\n",
      " [1.  0.  0.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  0.3]]\n",
      "(1, 49)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOsAAADrCAYAAACICmHVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAE+ElEQVR4nO3dMW4UdxjG4W9GkaZxKtOwcmGa5QDkNBwAH2B9ANJQbecKHwAuwwXihsZaN4lEgZC24Z+CKmDv2knsv1/meUrW4h2EfjBbfUNrrYDHb+z9AMDtiBVCiBVCiBVCiBVCiBVC/HKXHx6GJ63q+J4eZbfj479qu9122Z6mybbtB/Hp06f68uXLcN1nd4r1W6gf/vsT/Qtv3ryvi4uLLtvL5dK27Qfx9u3bGz/zGgwhxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAoh7nSY6unTTZ2c/H5fz7LHstMuPA57Yx2G4VVVvaqqOjw8rOWyTzTTNNm2/dNv77I31tbaeVWdV1UtFos2xzN8tm0/Br6zQgixQgixQgixQgixQgixQgixQgixQgixQgixQgixQgixQgixQgixQgixQgixQgixQgixQgixQgixQgixQgixQgixQgixQgixQgixQgixQgixQog7nXzsabPZ1OvXr7tsr9dr2zPbfoyG1truH/jnyccXZ2dnD/FcPxjHsS4vL7tsHx0d2Z7Z9tevX7tsr1ar2mw2w3WfxZx8PDg4qNPT0y7b6/Xa9sy2P3/+3GV7F99ZIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIUTMyceeFotFt/ODtvts9zrAtsveWL87+VjL5fLeH+o64zh2u5s5TVO3P7fteW3v4uTjLbx7967bv7TL5dL2jLZ38Z0VQogVQogVQogVQogVQogVQogVQogVQogVQogVQogVQogVQogVQogVQogVQogVQogVQogVQogVQogVQogVQogVQogVQogVQogVQogVQogVQogVQogVQjj5eAtzPT84TVMdHBx02R7HcZbbV1dXN342tNZu/RstFot2cnLyfzzTnTn52Gf75cuXXbbX63W3v++e21VVrbXhul/3GgwhxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAoh9l6R++7k44uzs7OHeK4fjONYl5eXXbaPjo66bT979qy2222X7WmabD+w1WpVm83m2itye++zttbOq+q86tvJx17nB3uefOx5AnDO5ybnuL2L12AIIVYIIVYIIVYIIVYIIVYIIVYIIVYIIVYIIVYIIVYIIVYIIVYIIVYIIVYIIVYIIVYIIVYIIVYIIVYIIVYIIVYIIVYIIVYIIVYIIVYIIVYIIVYIIVYIEXPyca4nAG332f64/dhl+3R1Wm3Tsk8+zvUEoO0+26cXfU587uI1GEKIFUKIFUKIFUKIFUKIFUKIFUKIFUKIFUKIFUKIFUKIFUKIFUKIFUKIFUKIFUKIFUKIFUKIFUKIFUKIFUKIFUKIFUKIFUKIFUKIFUKIFUKIFUKIFULc6eRjVT2vqj/u+6Fu8KSq/rRt+yffft5a+/W6D/bG+lgMw/Chtfabbdtz3fYaDCHECiGSYj23bXvO2zHfWWHukv5nhVkTK4QQK4QQK4QQK4T4G3mGiGRQ5PKsAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# canvas, reward, game_over = grid_world.act(UP)\n",
    "render(grid_world)\n",
    "print(grid_world.grid)\n",
    "print(grid_world.observe().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "857bf9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(model, env):\n",
    "    env.reset()\n",
    "    envstate = env.observe()\n",
    "    while True:\n",
    "        prev_envstate = envstate\n",
    "        # get next action\n",
    "        q = model.predict(prev_envstate)\n",
    "        action = np.argmax(q[0])\n",
    "\n",
    "        # apply action, get rewards and new state\n",
    "        envstate, reward, game_status = env.act(action)\n",
    "        if game_status == 'win':\n",
    "            return True\n",
    "        elif game_status == 'lose':\n",
    "            return False\n",
    "\n",
    "def completion_check(env):\n",
    "    if not env.get_valid_actions(env.start):\n",
    "        return False\n",
    "    if not env.get_valid_actions(env.destination):\n",
    "        return False\n",
    "    return True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4546d33d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion_check(grid_world)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60f2f72",
   "metadata": {},
   "source": [
    "episode = [envstate, action, reward, envstate_next, game_over]\n",
    "\n",
    "    - envstate : environment state 임. 모든 cell 의 상태를 의미. neural network 를 더 쉽게 하기 위해서, grid 를 1차원 vector 로 넣을 거임\n",
    "    - action : 4개의 action 중 하나 (0,1,2,3)\n",
    "    - reward : action 에서 얻은 reward\n",
    "    - envstate_next : last action 에 의해 transition 된 grid environment\n",
    "    - game_over : True 또는 False. game 이 끝났는지를 가리킴. Agent 가 destination cell 에 도달하거나, negative reward limit 에 도달하면 끝남. 각각 win 과 lose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25a9df68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory for Episodes\n",
    "class Experience(object):\n",
    "    def __init__(self, model, max_memory=100, discount=0.95):\n",
    "        self.model = model\n",
    "        self.max_memory = max_memory\n",
    "        self.discount = discount\n",
    "        self.memory = list()\n",
    "        self.num_actions = model.output_shape[-1]\n",
    "\n",
    "    def remember(self, episode):\n",
    "        # episode = [envstate, action, reward, envstate_next, game_over]\n",
    "        # memory[i] = episode\n",
    "        # envstate == flattened 1d grid cells info, including rat cell (see method: observe)\n",
    "        self.memory.append(episode)\n",
    "        if len(self.memory) > self.max_memory:\n",
    "            del self.memory[0]\n",
    "\n",
    "    def predict(self, envstate):\n",
    "        return self.model.predict(envstate)[0]\n",
    "\n",
    "    def get_data(self, data_size=10):\n",
    "        env_size = self.memory[0][0].shape[1]   # envstate 1d size (1st element of episode), 첫번째 episode 의 첫번째 element(envstate[1,64])\n",
    "        mem_size = len(self.memory)\n",
    "        data_size = min(mem_size, data_size)\n",
    "        inputs = np.zeros((data_size, env_size))\n",
    "        targets = np.zeros((data_size, self.num_actions))\n",
    "        for i, j in enumerate(np.random.choice(range(mem_size), data_size, replace=False)):\n",
    "            envstate, action, reward, envstate_next, game_over = self.memory[j]\n",
    "            inputs[i] = envstate\n",
    "            # There should be no target values for actions not taken.\n",
    "            targets[i] = self.predict(envstate)\n",
    "            # Q_sa = derived policy = max quality env/action = max_a' Q(s', a')\n",
    "            Q_sa = np.max(self.predict(envstate_next))\n",
    "            if game_over:\n",
    "                targets[i, action] = reward\n",
    "            else:\n",
    "                # reward + gamma * max_a' Q(s', a')\n",
    "                targets[i, action] = reward + self.discount * Q_sa\n",
    "        return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0de56ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(env, lr=0.001):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(env.grid.size, input_shape=(env.grid.size,)))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dense(env.grid.size))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dense(ACTION_NUMS))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3744c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qtrain(model, epochs, max_memory, data_size, weights_file = \"\", name = \"\"):\n",
    "    global epsilon\n",
    "#     n_epoch = opt.get('epochs', 15000)\n",
    "#     max_memory = opt.get('max_memory', 1000)\n",
    "#     data_size = opt.get('data_size', 50)\n",
    "#     weights_file = opt.get('weights_file', \"\")\n",
    "#     name = opt.get('name', 'model')\n",
    "    \n",
    "    n_epoch = epochs\n",
    "    max_memory = max_memory\n",
    "    data_size = data_size\n",
    "    weights_file = weights_file\n",
    "    name = name\n",
    "    start_time = datetime.datetime.now()\n",
    "\n",
    "    # If you want to continue training from a previous model,\n",
    "    # just supply the h5 file name to weights_file option\n",
    "    if weights_file:\n",
    "        print(\"loading weights from file: %s\" % (weights_file,))\n",
    "        model.load_weights(weights_file)\n",
    "\n",
    "    # Construct environment\n",
    "    env = Environment()\n",
    "    while not completion_check(env):\n",
    "        env = Environment()\n",
    "\n",
    "    # Initialize experience replay object\n",
    "    experience = Experience(model, max_memory=max_memory)\n",
    "\n",
    "    win_history = []   # history of win/lose game\n",
    "    hsize = env.grid.size//2   # history window size\n",
    "    win_rate = 0.0\n",
    "\n",
    "    for epoch in range(n_epoch):\n",
    "        loss = 0.0\n",
    "        env.reset()\n",
    "        while not completion_check(env):\n",
    "            env.reset()\n",
    "        \n",
    "        game_over = False\n",
    "\n",
    "        # get initial envstate (1d flattened canvas)\n",
    "        envstate = env.observe()\n",
    "\n",
    "        n_episodes = 0\n",
    "        while not game_over:\n",
    "            valid_actions = env.get_valid_actions()\n",
    "            if not valid_actions: break\n",
    "            prev_envstate = envstate\n",
    "            # Get next action\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = random.choice(valid_actions)\n",
    "            else:\n",
    "                action = np.argmax(experience.predict(prev_envstate))\n",
    "\n",
    "            # Apply action, get reward and new envstate\n",
    "            envstate, reward, game_status = env.act(action)\n",
    "            if game_status == 'win':\n",
    "                win_history.append(1)\n",
    "                game_over = True\n",
    "            elif game_status == 'lose':\n",
    "                win_history.append(0)\n",
    "                game_over = True\n",
    "            else:\n",
    "                game_over = False\n",
    "\n",
    "            # Store episode (experience)\n",
    "            episode = [prev_envstate, action, reward, envstate, game_over]\n",
    "            experience.remember(episode)\n",
    "            n_episodes += 1\n",
    "\n",
    "            # Train neural network model\n",
    "            inputs, targets = experience.get_data(data_size=data_size)\n",
    "            h = model.fit(\n",
    "                inputs,\n",
    "                targets,\n",
    "                nb_epoch=8,\n",
    "                batch_size=16,\n",
    "                verbose=0,\n",
    "            )\n",
    "            loss = model.evaluate(inputs, targets, verbose=0)\n",
    "\n",
    "        if len(win_history) > hsize:\n",
    "            win_rate = sum(win_history[-hsize:]) / hsize\n",
    "    \n",
    "        dt = datetime.datetime.now() - start_time\n",
    "        t = format_time(dt.total_seconds())\n",
    "        template = \"Epoch: {:03d}/{:d} | Loss: {:.4f} | Episodes: {:d} | Win count: {:d} | Win rate: {:.3f} | time: {}\"\n",
    "        print(template.format(epoch, n_epoch-1, loss, n_episodes, sum(win_history), win_rate, t))\n",
    "        # we simply check if training has exhausted all free cells and if in all\n",
    "        # cases the agent won\n",
    "        if win_rate > 0.9 : epsilon = 0.05\n",
    "        if sum(win_history[-hsize:]) == hsize and completion_check(env):\n",
    "            print(\"Reached 100%% win rate at epoch: %d\" % (epoch,))\n",
    "            break\n",
    "\n",
    "    # Save trained model weights and architecture, this will be used by the visualization code\n",
    "    h5file = name + \".h5\"\n",
    "    json_file = name + \".json\"\n",
    "    model.save_weights(h5file, overwrite=True)\n",
    "    with open(json_file, \"w\") as outfile:\n",
    "        json.dump(model.to_json(), outfile)\n",
    "    end_time = datetime.datetime.now()\n",
    "    dt = datetime.datetime.now() - start_time\n",
    "    seconds = dt.total_seconds()\n",
    "    t = format_time(seconds)\n",
    "    print('files: %s, %s' % (h5file, json_file))\n",
    "    print(\"n_epoch: %d, max_mem: %d, data: %d, time: %s\" % (epoch, max_memory, data_size, t))\n",
    "    return \n",
    "\n",
    "# This is a small utility for printing readable time strings:\n",
    "def format_time(seconds):\n",
    "    if seconds < 400:\n",
    "        s = float(seconds)\n",
    "        return \"%.1f seconds\" % (s,)\n",
    "    elif seconds < 4000:\n",
    "        m = seconds / 60.0\n",
    "        return \"%.2f minutes\" % (m,)\n",
    "    else:\n",
    "        h = seconds / 3600.0\n",
    "        return \"%.2f hours\" % (h,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2224627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000/999 | Loss: 0.0373 | Episodes: 71 | Win count: 1 | Win rate: 0.000 | time: 3.3 seconds\n",
      "Epoch: 001/999 | Loss: 0.0273 | Episodes: 109 | Win count: 1 | Win rate: 0.000 | time: 9.3 seconds\n",
      "Epoch: 002/999 | Loss: 0.0049 | Episodes: 90 | Win count: 2 | Win rate: 0.000 | time: 14.5 seconds\n",
      "Epoch: 003/999 | Loss: 0.0063 | Episodes: 109 | Win count: 2 | Win rate: 0.000 | time: 21.0 seconds\n",
      "Epoch: 004/999 | Loss: 0.0180 | Episodes: 22 | Win count: 3 | Win rate: 0.000 | time: 22.3 seconds\n",
      "Epoch: 005/999 | Loss: 0.0039 | Episodes: 22 | Win count: 4 | Win rate: 0.000 | time: 23.6 seconds\n",
      "Epoch: 006/999 | Loss: 0.0153 | Episodes: 116 | Win count: 4 | Win rate: 0.000 | time: 30.6 seconds\n",
      "Epoch: 007/999 | Loss: 0.0045 | Episodes: 34 | Win count: 5 | Win rate: 0.000 | time: 32.6 seconds\n",
      "Epoch: 008/999 | Loss: 0.0092 | Episodes: 18 | Win count: 6 | Win rate: 0.000 | time: 33.7 seconds\n",
      "Epoch: 009/999 | Loss: 0.0149 | Episodes: 18 | Win count: 7 | Win rate: 0.000 | time: 34.8 seconds\n",
      "Epoch: 010/999 | Loss: 0.0464 | Episodes: 20 | Win count: 8 | Win rate: 0.000 | time: 36.1 seconds\n",
      "Epoch: 011/999 | Loss: 0.0024 | Episodes: 22 | Win count: 9 | Win rate: 0.000 | time: 37.4 seconds\n",
      "Epoch: 012/999 | Loss: 0.0023 | Episodes: 21 | Win count: 10 | Win rate: 0.000 | time: 38.6 seconds\n",
      "Epoch: 013/999 | Loss: 0.0014 | Episodes: 20 | Win count: 11 | Win rate: 0.000 | time: 39.8 seconds\n",
      "Epoch: 014/999 | Loss: 0.0063 | Episodes: 20 | Win count: 12 | Win rate: 0.000 | time: 41.0 seconds\n",
      "Epoch: 015/999 | Loss: 0.0015 | Episodes: 69 | Win count: 13 | Win rate: 0.000 | time: 45.1 seconds\n",
      "Epoch: 016/999 | Loss: 0.0035 | Episodes: 58 | Win count: 14 | Win rate: 0.000 | time: 48.6 seconds\n",
      "Epoch: 017/999 | Loss: 0.0025 | Episodes: 18 | Win count: 15 | Win rate: 0.000 | time: 49.7 seconds\n",
      "Epoch: 018/999 | Loss: 0.0040 | Episodes: 25 | Win count: 16 | Win rate: 0.000 | time: 51.1 seconds\n",
      "Epoch: 019/999 | Loss: 0.0020 | Episodes: 18 | Win count: 17 | Win rate: 0.000 | time: 52.2 seconds\n",
      "Epoch: 020/999 | Loss: 0.0027 | Episodes: 24 | Win count: 18 | Win rate: 0.000 | time: 53.6 seconds\n",
      "Epoch: 021/999 | Loss: 0.0023 | Episodes: 25 | Win count: 19 | Win rate: 0.000 | time: 55.1 seconds\n",
      "Epoch: 022/999 | Loss: 0.0018 | Episodes: 20 | Win count: 20 | Win rate: 0.000 | time: 56.2 seconds\n",
      "Epoch: 023/999 | Loss: 0.0028 | Episodes: 26 | Win count: 21 | Win rate: 0.000 | time: 57.8 seconds\n",
      "Epoch: 024/999 | Loss: 0.0060 | Episodes: 20 | Win count: 22 | Win rate: 0.875 | time: 59.0 seconds\n",
      "Epoch: 025/999 | Loss: 0.0036 | Episodes: 18 | Win count: 23 | Win rate: 0.917 | time: 60.0 seconds\n",
      "Epoch: 026/999 | Loss: 0.0011 | Episodes: 23 | Win count: 24 | Win rate: 0.917 | time: 61.4 seconds\n",
      "Epoch: 027/999 | Loss: 0.0012 | Episodes: 18 | Win count: 25 | Win rate: 0.958 | time: 62.4 seconds\n",
      "Epoch: 028/999 | Loss: 0.0013 | Episodes: 18 | Win count: 26 | Win rate: 0.958 | time: 63.4 seconds\n",
      "Epoch: 029/999 | Loss: 0.0008 | Episodes: 18 | Win count: 27 | Win rate: 0.958 | time: 64.4 seconds\n",
      "Epoch: 030/999 | Loss: 0.0010 | Episodes: 58 | Win count: 28 | Win rate: 1.000 | time: 67.8 seconds\n",
      "Reached 100% win rate at epoch: 30\n",
      "files: result_weight.h5, result_weight.json\n",
      "n_epoch: 30, max_mem: 392, data: 32, time: 67.9 seconds\n"
     ]
    }
   ],
   "source": [
    "temp_env = Environment()\n",
    "model = build_model(temp_env)\n",
    "qtrain(model, epochs=1000, max_memory=8*temp_env.grid.size, data_size=32, name=\"result_weight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e53f0275",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "561e7380",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def demonstrate(model, epoch = 2):\n",
    "    for i in range(epoch):\n",
    "        rendered_imgs = []\n",
    "        count = 0\n",
    "        \n",
    "        env = Environment()\n",
    "        while not completion_check(env):\n",
    "            env = Environment()\n",
    "        envstate = env.observe()\n",
    "        rendered_imgs.append(render(env))\n",
    "        plt.savefig(\"demo/\"+str(i)+\"-\"+str(count)+\".png\")\n",
    "        count += 1\n",
    "        while True:\n",
    "            prev_envstate = envstate\n",
    "            q = model.predict(prev_envstate)\n",
    "            action = np.argmax(q[0])\n",
    "            envstate, reward, game_status = env.act(action)\n",
    "            print(count, action)\n",
    "            rendered_imgs.append(render(env))\n",
    "            plt.savefig(\"demo/\"+str(i)+\"-\"+str(count)+\".png\")\n",
    "            count += 1\n",
    "            if game_status == 'win':\n",
    "                print(\"win\")\n",
    "                break\n",
    "            elif game_status == 'lose':\n",
    "                print(\"lose\")\n",
    "                break\n",
    "#         while True:\n",
    "#             prev_envstate = envstate\n",
    "#             # get next action\n",
    "#             q = model.predict(prev_envstate)\n",
    "#             action = np.argmax(q[0])\n",
    "\n",
    "#             # apply action, get rewards and new state\n",
    "#             envstate, reward, game_status = env.act(action)\n",
    "#             rendered_imgs.append(render(env))\n",
    "#             if game_status == 'win':\n",
    "#                 break\n",
    "#             elif game_status == 'lose':\n",
    "#                 break\n",
    "        print(len(rendered_imgs))\n",
    "#         count = 0\n",
    "#         for img in rendered_imgs:\n",
    "#             plt.savefig(img, \"demo/\"+str(i)+\"-\"+str(count)+\".png\")\n",
    "#             count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d4c173d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2\n",
      "2 3\n",
      "3 2\n",
      "4 2\n",
      "5 2\n",
      "6 2\n",
      "7 2\n",
      "8 3\n",
      "9 3\n",
      "10 0\n",
      "11 0\n",
      "12 0\n",
      "13 3\n",
      "14 3\n",
      "15 2\n",
      "16 3\n",
      "17 2\n",
      "18 2\n",
      "win\n",
      "19\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "demonstrate(model, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84df30e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e7ecde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3dd876",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jjs2",
   "language": "python",
   "name": "jjs2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
